{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aviso importante!\n",
    "\n",
    "O objetivo principal desse projeto é apenas mostrar que é possível utilizar machine learning para auxiliar o investidor na sua tomada de decisão.\n",
    "\n",
    "Esse projeto não é uma indicação de investimento! Toda estratégia e resultados apresentados **são apenas para fins didáticos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instalação inicial das bibliotecas\n",
    "!pip install yfinance\n",
    "!pip install investpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install plotly\n",
    "!pip install seaborn\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance\n",
    "import investpy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn import linear_model\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Coleta dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa etapa será utilizado o método **download()** da biblioteca **yfinance** para obter os dados de determinada ação.\n",
    "\n",
    "Esse método retorna um **dataframe** com as devidas colunas referentes à negociação feita em cada dia em que a ação foi negociada na bolsa de valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori, serão utilizados os dados da empresa ITAU mas outras podem ser analisadas, basta alterar o ticker da ação.\n",
    "\n",
    "Ticker analisado: ITUB4.SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Obtendo dataset da ação (01/01/2016 até 30/06/21) \n",
    "ticker = \"ITUB4.SA\"\n",
    "df = yfinance.download(ticker, start=\"2016-01-01\", end=\"2021-06-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtendo dataset do dolar (01/01/2016 até 30/06/21) \n",
    "dolar = investpy.get_currency_cross_historical_data(currency_cross='USD/BRL', from_date='01/01/2016', to_date='30/06/2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tratamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Verificando dataset da ação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantidade de registros do dataset da ação\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identificando dados faltantes\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Verificando dataset dolar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantidade de registros do dataset da dólar\n",
    "dolar.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identificando dados faltantes\n",
    "dolar.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Integrando os datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao importar os dados das ações e do dólar possuem o campo Date como index. Utilizamos o **reset_index()** para modificar o index do dataset sem excluir a coluna (**drop=False**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#necessário fazer o reset_index pois o dataset vem indexado pela data\n",
    "df = df.reset_index(drop=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar = dolar.reset_index(drop=False)\n",
    "dolar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Juntando os dois datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o objetivo é realizar uma tarefa de predição atributos não numéricos não serão utilizados. Dessa forma, até para poupar processamento, a coluna **Currency** será eliminada do dataset do dólar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa etapa será realizado a integração dos dois datasets em apenas um. Para isso modificamos o nome dos atributos do dólar para facilitar a identificação no dataset integrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolar = dolar.drop('Currency', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dolar = dolar.rename(columns={\"Open\": \"Open_dolar\",\n",
    "                                 \"High\": \"High_dolar\",\n",
    "                                 \"Low\": \"Low_dolar\",\n",
    "                                 \"Close\": \"Close_dolar\"})\n",
    "df_dolar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.merge(df, df_dolar, on='Date', how='inner')\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tamanho do dataset integrado\n",
    "df_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando o último registro no dataset integrado (29/06/2021).\n",
    "df_full.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importante registrar em uma outra variável o último registro, no caso o dia 29/06/2021, pois após treinar os modelos tentaremos prever o valor do Adj Close desse dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultimo_registro = df_full.tail(1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualização de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráfico de candlestick para ação\n",
    "# criamos uma variável, do tipo dicionário (chave e valor), para personalizar a lista de impressão no gráfico.\n",
    "trace1 = {\n",
    "    'x': df_full.Date,\n",
    "    'open': df_full.Open,\n",
    "    'close': df_full.Close,\n",
    "    'high': df_full.High,\n",
    "    'low': df_full.Low,\n",
    "    'type': 'candlestick',\n",
    "    'name': 'ITUB4.SA',\n",
    "    'showlegend': True\n",
    "}\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout()\n",
    "\n",
    "fig1 = go.Figure(data=data, layout=layout)\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráfico de candlestick do dolar\n",
    "trace2 = {\n",
    "    'x': df_full.Date,\n",
    "    'open': df_full.Open_dolar,\n",
    "    'close': df_full.Close_dolar,\n",
    "    'high': df_full.High_dolar,\n",
    "    'low': df_full.Low_dolar,\n",
    "    'type': 'candlestick',\n",
    "    'name': 'Dólar',\n",
    "    'showlegend': True\n",
    "}\n",
    "\n",
    "data = [trace2]\n",
    "layout = go.Layout()\n",
    "\n",
    "fig2 = go.Figure(data=data, layout=layout)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_full.corr(), annot = False) # O parâmetro annot = True faz com que os valores fiquem visíveis no mapa.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6)) #altura e largura do gráfico\n",
    "plt.plot(df_full['Date'], df_full['Adj Close'])\n",
    "plt.plot(df_full['Date'], df_full['Close_dolar'])\n",
    "plt.legend(['Adj Close', 'Close_dolar'])\n",
    "plt.grid()\n",
    "plt.title(\"Cotação da ação e dólar x Tempo\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Preparando dataset para os algoritmos de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deslocando os valores da coluna 'Adj Close' para cima utilizando o método shift(-1).\n",
    "# isso é necessário para verificar a previsão do próximo dia, utilizando as features ou características do dia atual.\n",
    "\n",
    "df_full['Adj Close'] = df_full['Adj Close'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando novo valor do dia 29-06-2021 (deve ser NaN)\n",
    "df_full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.dropna()\n",
    "df_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultimo_registro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Selecionando features com KBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_full.drop(['Date','Adj Close'],1)\n",
    "label = df_full['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = ('Open','High','Low','Close','Volume', 'Open_dolar','High_dolar','Low_dolar','Close_dolar')\n",
    "\n",
    "k_best_features = SelectKBest(k='all')\n",
    "k_best_features.fit_transform(features, label)\n",
    "k_best_features_scores = k_best_features.scores_\n",
    "raw_pairs = zip(features_list[1:], k_best_features_scores)\n",
    "ordered_pairs = list(reversed(sorted(raw_pairs, key=lambda x: x[1])))\n",
    "\n",
    "k_best_features_final = dict(ordered_pairs[:15])\n",
    "best_features = k_best_features_final.keys()\n",
    "print ('')\n",
    "print (\"Melhores features:\")\n",
    "print (k_best_features_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separando as features escolhidas\n",
    "features = df_full.loc[:,['Low_dolar', 'Close_dolar', 'Close','Volume', 'High_dolar']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Separando o dataset para treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtd_linhas = len(df_full)\n",
    "\n",
    "qtd_linhas_treino= round(.70 * qtd_linhas)\n",
    "qtd_linhas_teste= qtd_linhas - qtd_linhas_treino  \n",
    "\n",
    "print(\"Quant linhas de treino: 0:\"+ str(qtd_linhas_treino))\n",
    "print(\"Quant linhas de teste: \" + str(qtd_linhas_treino) + \":\" + str(qtd_linhas_treino + qtd_linhas_teste -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separa os dados de treino e teste\n",
    "X_train = features[:qtd_linhas_treino]\n",
    "X_test = features[qtd_linhas_treino:qtd_linhas_treino + qtd_linhas_teste -1]\n",
    "\n",
    "y_train = label[:qtd_linhas_treino]\n",
    "y_test = label[qtd_linhas_treino:qtd_linhas_treino + qtd_linhas_teste -1]\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4. Normalizando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma importante etapa é o processo de normalização dos dados, para que o treinamento dos modelos sejam feitos com todas as features (características/colunas) dentro de uma mesma escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)  # Normalizando os dados de entrada(treinamento)\n",
    "X_test_scale  = scaler.transform(X_test)       # Normalizando os dados de entrada(teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Previsão com Regressão Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treinamento usando regressão linear\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(X_train_scale, y_train)\n",
    "pred_lr= lr.predict(X_test_scale)\n",
    "cd =r2_score(y_test, pred_lr)\n",
    "rmse_lr = sqrt(mean_squared_error(y_test, pred_lr))\n",
    "\n",
    "print('------Linear Regression------')\n",
    "print(f'Coeficiente de determinação:{cd * 100:.2f}')\n",
    "print(f'RMSE: {rmse_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Previsão com ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide o dataset em 70:30 \n",
    "training_data = y_train.values\n",
    "test_data = y_test.values\n",
    "\n",
    "history = [x for x in training_data]\n",
    "model_predictions = []\n",
    "N_test_observations = len(test_data)\n",
    "for time_point in range(N_test_observations):\n",
    "    model = ARIMA(history, order=(6,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    model_predictions.append(yhat)\n",
    "    true_test_value = test_data[time_point]\n",
    "    history.append(true_test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_arima =r2_score(test_data, np.array(model_predictions))\n",
    "RMSE_error = sqrt(mean_squared_error(test_data, np.array(model_predictions)))\n",
    "print('------ARIMA------')\n",
    "print(f'Coeficiente de determinação: {cd_arima * 100:.2f}')\n",
    "print(f'RMSE: {RMSE_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Previsão com MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.1. MLP com configuração padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rede neural\n",
    "rn = MLPRegressor(max_iter=1000)\n",
    "\n",
    "rn.fit(X_train_scale, y_train)\n",
    "pred_rn= rn.predict(X_test_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cd_mlp1 = rn.score(X_test_scale, y_test)\n",
    "rmse_mlp1 = sqrt(mean_squared_error(pred_rn, y_test))\n",
    "\n",
    "print('------MLP------')\n",
    "print(f'Coeficiente de determinação:{cd * 100:.2f}')\n",
    "print(f'Root Mean Squared Error is {rmse_mlp1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.2. MLP com configuração de ajuste de hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn_mlp = MLPRegressor(max_iter=1000)\n",
    "\n",
    "parameter_space = {\n",
    "        'hidden_layer_sizes': [(i,) for i in list(range(1, 21))],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam', 'lbfgs'], \n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "    }\n",
    "\n",
    "search = GridSearchCV(rn_mlp, parameter_space, n_jobs=-1, cv=5)\n",
    "\n",
    "\n",
    "search.fit(X_train_scale,y_train)\n",
    "clf = search.best_estimator_\n",
    "pred_mlp= search.predict(X_test_scale)\n",
    "\n",
    "cd = search.score(X_test_scale, y_test)\n",
    "rmse_mlp2 = sqrt(mean_squared_error(pred_mlp, y_test))\n",
    "\n",
    "print(f'Coeficiente de determinação:{cd * 100:.2f}')\n",
    "print(f'Root Mean Squared Error is {rmse_mlp2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para executar a etapa de validação, será utilizado as features do dia 29/06/2021, pois os modelos foram treinados e testados com os dados históricos até dia 28/06/2021.\n",
    "\n",
    "O objetivo é tentar prever o \"Adj Close\" da ação um dia a frente. No caso o dia a frente é o dia 29/06.\n",
    "\n",
    "Portanto, para obter o resultado da validação, os modelos treinados devem obter um valor de predição próximo à \"Adj Close = 30.119904\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copiando as mesmas features escolhidas\n",
    "valor_novo = ultimo_registro.loc[:,['Low_dolar', 'Close_dolar', 'Close','Volume', 'High_dolar']].copy()\n",
    "valor_novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#executando a predição\n",
    "\n",
    "print(\"Valor real: 30.119904\")\n",
    "\n",
    "previsao=scaler.transform(valor_novo)\n",
    "\n",
    "pred=lr.predict(previsao)\n",
    "print(f'Valor predito pela Regressão Linear {pred}')\n",
    "\n",
    "print(f'Valor predito pelo ARIMA {yhat}')\n",
    "\n",
    "pred=rn.predict(previsao)\n",
    "print(f'Valor predito pela MLP {pred}')\n",
    "\n",
    "pred=search.predict(previsao)\n",
    "print(print(f'Valor predito pela MLP Ajustado {pred}'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
